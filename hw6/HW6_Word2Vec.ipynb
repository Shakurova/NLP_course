{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-23 02:32:42,681 : INFO : Loading dictionaries from /usr/local/lib/python3.5/site-packages/pymorphy2_dicts/data\n",
      "2017-03-23 02:32:43,079 : INFO : format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "import gensim, logging\n",
    "\n",
    "import pymorphy2\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "RUS_LETTERS = u'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала нужно провести предобработку текстов: лемматизировать, удалить стоп-слова, возможно, разметить части речи (но последнее необязательно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anekdots = [f for f in os.listdir('./weka/anekdots/') if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "izvest = [f for f in os.listdir('./weka/izvest/') if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teh_mol = [f for f in os.listdir('./weka/teh_mol/') if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anekdots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-23 02:32:48,225 : INFO : loading projection weights from ruscorpora_1_300_10.bin\n",
      "2017-03-23 02:32:54,408 : INFO : loaded (184973, 300) matrix from ruscorpora_1_300_10.bin\n"
     ]
    }
   ],
   "source": [
    "m = 'ruscorpora_1_300_10.bin'\n",
    "if m.endswith('.vec'):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=False)\n",
    "elif m.endswith('.bin'):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=True)\n",
    "else:\n",
    "    model = gensim.models.Word2Vec.load(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_corpora = []\n",
    "\n",
    "for a in anekdots:\n",
    "    with open('./weka/anekdots/' + a, 'r', encoding='utf-8') as r:\n",
    "        dicti = {}\n",
    "        text = r.read()\n",
    "        dicti['class'] = 'anekdots'\n",
    "        dicti['text'] = text\n",
    "        arr_corpora.append(dicti)\n",
    "\n",
    "for i in izvest:\n",
    "    with open('./weka/izvest/' + i, 'r', encoding='utf-8') as r:\n",
    "        dicti = {}\n",
    "        text = r.read()\n",
    "        dicti['class'] = 'izvest'\n",
    "        dicti['text'] = text\n",
    "        arr_corpora.append(dicti)\n",
    "        \n",
    "for t in teh_mol:\n",
    "    with open('./weka/teh_mol/' + t, 'r', encoding='utf-8') as r:\n",
    "        dicti = {}\n",
    "        text = r.read()\n",
    "        dicti['class'] = 'teh_mol'\n",
    "        dicti['text'] = text\n",
    "        arr_corpora.append(dicti)\n",
    "        \n",
    "        \n",
    "random.shuffle(arr_corpora)  # перемешиваем данные\n",
    "    \n",
    "df = pd.DataFrame(arr_corpora)  # создаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train, data_test, class_train, class_test = train_test_split(df['text'], df['class'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>teh_mol</td>\n",
       "      <td>Однажды...\\nИ хватит об этом!\\nКак-то раз берл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>teh_mol</td>\n",
       "      <td>Сергей Алексеев. Тропы еще в антимир не протоп...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>izvest</td>\n",
       "      <td>Лесков Сергей. Калмыкию оставили без космодром...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anekdots</td>\n",
       "      <td>Sех, он и она\\n***\\nНа мосту стояли трое: он, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>izvest</td>\n",
       "      <td>Анненков Андрей. Горячие кристаллы на остывающ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                               text\n",
       "0   teh_mol  Однажды...\\nИ хватит об этом!\\nКак-то раз берл...\n",
       "1   teh_mol  Сергей Алексеев. Тропы еще в антимир не протоп...\n",
       "2    izvest  Лесков Сергей. Калмыкию оставили без космодром...\n",
       "3  anekdots  Sех, он и она\\n***\\nНа мосту стояли трое: он, ...\n",
       "4    izvest  Анненков Андрей. Горячие кристаллы на остывающ..."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_smth_with_model(steps):\n",
    "    print('\\nModel train')\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    cv_results = cross_val_score(pipeline,\n",
    "                                 data_train,\n",
    "                                 class_train,\n",
    "                                 cv=10,\n",
    "                                 scoring='accuracy',\n",
    "                                )\n",
    "    print(cv_results.mean(), cv_results.std())\n",
    "\n",
    "    pipeline.fit(data_train, class_train)\n",
    "    class_predicted = pipeline.predict(data_test)\n",
    "    print(class_predicted)\n",
    "\n",
    "    print(classification_report(class_test, class_predicted ))\n",
    "\n",
    "    return pipeline, class_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transit = {'ADJF':'ADJ',\n",
    "'ADJS' : 'ADJ',\n",
    "'ADVB' : 'ADV',\n",
    "'COMP' : 'ADV',\n",
    "'CONJ' : 'CCONJ',\n",
    "'GRND' : 'VERB',\n",
    "'INFN' : 'VERB',\n",
    "'INTJ' : 'INTJ',\n",
    "'LATN' : 'X',\n",
    "'NOUN' : 'NOUN',\n",
    "'NPRO' : 'PRON',\n",
    "'NUMB' : 'NUM',\n",
    "'NUMR' : 'NUM',\n",
    "'PNCT' : 'PUNCT' ,\n",
    "'PRCL' : 'PART',\n",
    "'PRED' : 'ADV',\n",
    "'PREP' : 'ADP',\n",
    "'PRTF' : 'ADJ',\n",
    "'PRTS' : 'VERB',\n",
    "'ROMN' : 'X',\n",
    "'SYMB' : 'SYM',\n",
    "'UNKN' : 'X',\n",
    "'VERB' : 'VERB'}\n",
    "\n",
    "robj = re.compile('|'.join(transit.keys()))\n",
    "\n",
    "def cleanization(text):\n",
    "    for line in text:\n",
    "        # 1. Все буквы в нижний регистр\n",
    "        text_text = text.lower()\n",
    "\n",
    "        # 2. Удаление всех небукв\n",
    "        letters_only = ''\n",
    "        for _c in text_text:\n",
    "            if _c in RUS_LETTERS:\n",
    "                letters_only += _c\n",
    "            else:\n",
    "                letters_only += ' '\n",
    "\n",
    "        # 3. Заменяем множественные пробелы\n",
    "        while '  ' in letters_only:\n",
    "            letters_only = letters_only.replace('  ', ' ')\n",
    "\n",
    "        # 4. Токенизация\n",
    "        word_list = tokenizer.tokenize(letters_only)\n",
    "\n",
    "        # 5. Лемматизация\n",
    "        clean_word_list = [morph.parse(word)[0].normal_form for word in word_list]  # лемматизация\n",
    "    \n",
    "        # 6. * Удаление стоп-слов + добавление тегов - части речи\n",
    "        # meaningful_words = [word for word in clean_word_list if word not in get_stop_words('ru')] # стоп-слова\n",
    "        meaningful_words = [str(word) + '_' + robj.sub(lambda m: transit[m.group(0)], str(morph.parse(word)[0].tag.POS)) for word in clean_word_list]\n",
    "        return ' '.join(meaningful_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def mean(a):\n",
    "    return sum(a) / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word2vec_mean(text):\n",
    "    \"\"\"Сколько чисел.\"\"\"\n",
    "    arr = []\n",
    "    clean_text = cleanization(text)\n",
    "    # для каждого слова в тексте выводим его вектор\n",
    "    for word in clean_text.split(' '):\n",
    "    # есть ли слово в модели? Может быть, и нет\n",
    "        if word in model:\n",
    "            arr.append(model[word])\n",
    "    if len(list(map(mean, zip(*arr)))) != 0:\n",
    "        return list(map(mean, zip(*arr)))\n",
    "    else:\n",
    "        return [0 for i in range(0, 300)]\n",
    "\n",
    "\n",
    "\n",
    "class FunctionFeaturizer(TransformerMixin):\n",
    "    \"\"\" Для создания своего вектора я использовала вектор слова в модели word2vec\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        fvs = []\n",
    "        for datum in X:\n",
    "            fv = word2vec_mean(datum)\n",
    "            fvs.append(fv)\n",
    "        return np.array(fvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_featurizer = FunctionFeaturizer()  # создание своего векторизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# numbers('Для создания своего вектора я использовала несколько фич: длину текста, количество заглавных букв')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Transformer\n",
      "\n",
      "Model train\n",
      "0.88672877271 0.0263862200298\n",
      "['izvest' 'anekdots' 'izvest' 'anekdots' 'teh_mol' 'anekdots' 'teh_mol'\n",
      " 'anekdots' 'anekdots' 'izvest' 'anekdots' 'anekdots' 'izvest' 'izvest'\n",
      " 'anekdots' 'izvest' 'teh_mol' 'izvest' 'teh_mol' 'teh_mol' 'anekdots'\n",
      " 'teh_mol' 'teh_mol' 'anekdots' 'anekdots' 'teh_mol' 'anekdots' 'anekdots'\n",
      " 'anekdots' 'teh_mol' 'anekdots' 'teh_mol' 'izvest' 'teh_mol' 'teh_mol'\n",
      " 'anekdots' 'izvest' 'teh_mol' 'izvest' 'teh_mol' 'izvest' 'anekdots'\n",
      " 'izvest' 'izvest' 'izvest' 'teh_mol' 'anekdots' 'teh_mol' 'anekdots'\n",
      " 'izvest' 'anekdots' 'teh_mol' 'anekdots' 'teh_mol' 'izvest' 'anekdots'\n",
      " 'anekdots' 'anekdots' 'anekdots' 'anekdots' 'teh_mol' 'izvest' 'teh_mol'\n",
      " 'izvest' 'teh_mol' 'anekdots' 'anekdots' 'teh_mol' 'izvest' 'izvest'\n",
      " 'anekdots' 'izvest' 'izvest' 'izvest' 'anekdots']\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   anekdots       0.97      0.97      0.97        30\n",
      "     izvest       0.87      0.83      0.85        24\n",
      "    teh_mol       0.77      0.81      0.79        21\n",
      "\n",
      "avg / total       0.88      0.88      0.88        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Свой векторизатор\n",
    "print('\\nCustom Transformer')\n",
    "lg_pipeline, label_predicted = do_smth_with_model(steps=[('custom', w2v_featurizer),\n",
    "                                                      ('classifier', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "История произошла перед новым годом. Иду в Ашан закупаться, а за мной по тротуару следом два парня идут. И довольно громко обсуждают своего друга, причем довольно явно слышится кавказский акцент и характерные словечки, вроде \"брат\" при каждом обращении. Парни молодые, речь вдобавок с \"пацанскими\" оборотами. И тут звучит фраза, которую я совершенно не ожидал услышать: \"Брат, я ему и говорю - что за врач из тебя будет, если ты на нейроанатомию забиваешь?!\" И как-то тепло на душе стало, что это не просто пацан, а человек, уже понимающий ценность образования. : anekdots\n",
      "- А ты смогла бы всю жизнь прожить с одним мужчиной?- Да я-то смогла бы. Но мужика жалко. : anekdots\n"
     ]
    }
   ],
   "source": [
    "# Проверка работы модели на наших тестовых коллокациях\n",
    "def predictor(collocations_array, pipeline):\n",
    "    arr = []\n",
    "    df1 = pd.DataFrame({'text': collocations_array})\n",
    "    for i in df1.text:\n",
    "        arr.append(i)\n",
    "    с = 0\n",
    "    for i in pipeline.predict(df1.text):\n",
    "        print(arr[с], ':', i)\n",
    "        с += 1\n",
    "\n",
    "\n",
    "collocations_array = ['История произошла перед новым годом. Иду в Ашан закупаться, а за мной по тротуару следом два парня идут. И довольно громко обсуждают своего друга, причем довольно явно слышится кавказский акцент и характерные словечки, вроде \"брат\" при каждом обращении. Парни молодые, речь вдобавок с \"пацанскими\" оборотами. И тут звучит фраза, которую я совершенно не ожидал услышать: \"Брат, я ему и говорю - что за врач из тебя будет, если ты на нейроанатомию забиваешь?!\" И как-то тепло на душе стало, что это не просто пацан, а человек, уже понимающий ценность образования.', \n",
    "                      '- А ты смогла бы всю жизнь прожить с одним мужчиной?- Да я-то смогла бы. Но мужика жалко.']\n",
    "predictor(collocations_array, lg_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Transformer\n",
      "\n",
      "Model train\n"
     ]
    }
   ],
   "source": [
    "# Свой векторизатор\n",
    "print('\\nCustom Transformer')\n",
    "lg2_pipeline, label_predicted = do_smth_with_model(steps=[('custom', w2v_featurizer),\n",
    "                                                      ('classifier', LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=300, n_jobs=4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
