{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "import gensim, logging\n",
    "\n",
    "import pymorphy2\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "RUS_LETTERS = u'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала нужно провести предобработку текстов: лемматизировать, удалить стоп-слова, возможно, разметить части речи (но последнее необязательно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "anekdots = [f for f in os.listdir('./weka/anekdots/') if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "izvest = [f for f in os.listdir('./weka/izvest/') if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teh_mol = [f for f in os.listdir('./weka/teh_mol/') if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(anekdots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-17 22:57:56,480 : INFO : loading projection weights from ruscorpora_1_300_10.bin\n",
      "2017-03-17 22:58:01,913 : INFO : loaded (184973, 300) matrix from ruscorpora_1_300_10.bin\n"
     ]
    }
   ],
   "source": [
    "m = 'ruscorpora_1_300_10.bin'\n",
    "if m.endswith('.vec'):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=False)\n",
    "elif m.endswith('.bin'):\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(m, binary=True)\n",
    "else:\n",
    "    model = gensim.models.Word2Vec.load(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arr_corpora = []\n",
    "\n",
    "for a in anekdots:\n",
    "    with open('./weka/anekdots/' + a, 'r', encoding='utf-8') as r:\n",
    "        dicti = {}\n",
    "        text = r.read()\n",
    "        dicti['class'] = 'anekdots'\n",
    "        dicti['text'] = text\n",
    "        arr_corpora.append(dicti)\n",
    "\n",
    "for i in izvest:\n",
    "    with open('./weka/izvest/' + i, 'r', encoding='utf-8') as r:\n",
    "        dicti = {}\n",
    "        text = r.read()\n",
    "        dicti['class'] = 'izvest'\n",
    "        dicti['text'] = text\n",
    "        arr_corpora.append(dicti)\n",
    "        \n",
    "for t in teh_mol:\n",
    "    with open('./weka/teh_mol/' + t, 'r', encoding='utf-8') as r:\n",
    "        dicti = {}\n",
    "        text = r.read()\n",
    "        dicti['class'] = 'teh_mol'\n",
    "        dicti['text'] = text\n",
    "        arr_corpora.append(dicti)\n",
    "        \n",
    "        \n",
    "random.shuffle(arr_corpora)  # перемешиваем данные\n",
    "    \n",
    "df = pd.DataFrame(arr_corpora)  # создаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train, data_test, class_train, class_test = train_test_split(df['text'], df['class'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anekdots</td>\n",
       "      <td>Шлюхи\\n***\\nВ публичный дом приходит клиент и ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>anekdots</td>\n",
       "      <td>Ходжа Насреддин\\n***\\nНа вопрос соседа, что ну...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>izvest</td>\n",
       "      <td>Лесков Сергей. Электронный бум -- не фикция. Н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>anekdots</td>\n",
       "      <td>Хохлы\\n***\\nКупил новый украинец себе шестисот...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>teh_mol</td>\n",
       "      <td>Г. Малиничев. Десять заповедей инженера\\nНедав...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      class                                               text\n",
       "0  anekdots  Шлюхи\\n***\\nВ публичный дом приходит клиент и ...\n",
       "1  anekdots  Ходжа Насреддин\\n***\\nНа вопрос соседа, что ну...\n",
       "2    izvest  Лесков Сергей. Электронный бум -- не фикция. Н...\n",
       "3  anekdots  Хохлы\\n***\\nКупил новый украинец себе шестисот...\n",
       "4   teh_mol  Г. Малиничев. Десять заповедей инженера\\nНедав..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_smth_with_model(steps):\n",
    "    print('\\nModel train')\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "\n",
    "    cv_results = cross_val_score(pipeline,\n",
    "                                 data_train,\n",
    "                                 class_train,\n",
    "                                 cv=10,\n",
    "                                 scoring='accuracy',\n",
    "                                )\n",
    "    print(cv_results.mean(), cv_results.std())\n",
    "\n",
    "    pipeline.fit(data_train, class_train)\n",
    "    class_predicted = pipeline.predict(data_test)\n",
    "    print(class_predicted)\n",
    "\n",
    "    print(classification_report(class_test, class_predicted ))\n",
    "\n",
    "    return pipeline, class_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transit = {'ADJF':'ADJ', 'INFN':'VERB', 'ADVB':'ADV'}\n",
    "robj = re.compile('|'.join(transit.keys()))\n",
    "\n",
    "def cleanization(text):\n",
    "    for line in text:\n",
    "        # 1. Все буквы в нижний регистр\n",
    "        text_text = text.lower()\n",
    "\n",
    "        # 2. Удаление всех небукв\n",
    "        letters_only = ''\n",
    "        for _c in text_text:\n",
    "            if _c in RUS_LETTERS:\n",
    "                letters_only += _c\n",
    "            else:\n",
    "                letters_only += ' '\n",
    "\n",
    "        # 3. Заменяем множественные пробелы\n",
    "        while '  ' in letters_only:\n",
    "            letters_only = letters_only.replace('  ', ' ')\n",
    "\n",
    "        # 4. Токенизация\n",
    "        word_list = tokenizer.tokenize(letters_only)\n",
    "\n",
    "        # 5. Лемматизация\n",
    "        clean_word_list = [morph.parse(word)[0].normal_form for word in word_list]  # лемматизация\n",
    "    \n",
    "        # 6. Удаление стоп-слов + добавление тегов - части речи\n",
    "        meaningful_words = [str(word) + '_' + robj.sub(lambda m: transit[m.group(0)], str(morph.parse(word)[0].tag.POS)) for word in clean_word_list if word not in get_stop_words('ru')] # стоп-слова\n",
    "        return ' '.join(meaningful_words)   # meaningful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "def mean(a):\n",
    "    return sum(a) / len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numbers(text):\n",
    "    \"\"\"Сколько чисел.\"\"\"\n",
    "    arr = []\n",
    "    clean_text = cleanization(text)\n",
    "    # для каждого слова в тексте выводим его вектор\n",
    "    for word in clean_text.split():\n",
    "    # есть ли слово в модели? Может быть, и нет\n",
    "        if word in model:\n",
    "            arr.append(model[word])\n",
    "    # print(len(list(map(mean, zip(*arr)))))\n",
    "    return list(map(mean, zip(*arr)))\n",
    "\n",
    "\n",
    "\n",
    "class FunctionFeaturizer(TransformerMixin):\n",
    "    \"\"\" Для создания своего вектора я использовала несколько фич: длину текста, количество заглавных букв\n",
    "     (чем больше, тем обычно выше вероятность, что это спам), количество ! (в спам-сообщениях встречаются часто),\n",
    "     количество чисел, сколько слов из словаря спам-слов (50 самых частых слов в коллекции спам-сообщений)\"\"\"\n",
    "    def __init__(self, featurizers):\n",
    "        self.featurizers = featurizers\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        fvs = []\n",
    "        for datum in X:\n",
    "            fv = numbers(datum)\n",
    "            fvs.append(fv)\n",
    "        return np.array(fvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_featurizer = FunctionFeaturizer(numbers)  # создание своего векторизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# numbers('Для создания своего вектора я использовала несколько фич: длину текста, количество заглавных букв')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Transformer\n",
      "\n",
      "Model train\n",
      "0.860307749351 0.0796299815339\n",
      "['teh_mol' 'izvest' 'anekdots' 'anekdots' 'izvest' 'izvest' 'anekdots'\n",
      " 'anekdots' 'anekdots' 'anekdots' 'teh_mol' 'izvest' 'anekdots' 'izvest'\n",
      " 'anekdots' 'teh_mol' 'izvest' 'teh_mol' 'izvest' 'teh_mol' 'izvest'\n",
      " 'anekdots' 'anekdots' 'izvest' 'anekdots' 'izvest' 'izvest' 'anekdots'\n",
      " 'teh_mol' 'teh_mol' 'izvest' 'anekdots' 'teh_mol' 'teh_mol' 'izvest'\n",
      " 'teh_mol' 'izvest' 'teh_mol' 'teh_mol' 'teh_mol' 'anekdots' 'izvest'\n",
      " 'teh_mol' 'anekdots' 'izvest' 'anekdots' 'izvest' 'anekdots' 'anekdots'\n",
      " 'anekdots' 'izvest' 'izvest' 'izvest' 'izvest' 'teh_mol' 'izvest'\n",
      " 'anekdots' 'anekdots' 'anekdots' 'izvest' 'izvest' 'teh_mol' 'izvest'\n",
      " 'teh_mol' 'izvest' 'anekdots' 'teh_mol' 'teh_mol' 'anekdots' 'teh_mol'\n",
      " 'anekdots' 'anekdots' 'izvest' 'izvest' 'izvest']\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   anekdots       0.96      1.00      0.98        25\n",
      "     izvest       0.79      0.88      0.84        26\n",
      "    teh_mol       0.90      0.75      0.82        24\n",
      "\n",
      "avg / total       0.88      0.88      0.88        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Свой векторизатор\n",
    "print('\\nCustom Transformer')\n",
    "pipeline, label_predicted = do_smth_with_model(steps=[('custom', spam_featurizer),\n",
    "                                                      ('classifier', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Transformer\n",
      "\n",
      "Model train\n"
     ]
    }
   ],
   "source": [
    "# Свой векторизатор\n",
    "print('\\nCustom Transformer')\n",
    "pipeline, label_predicted = do_smth_with_model(steps=[('custom', spam_featurizer),\n",
    "                                                      ('classifier', LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=300, n_jobs=4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
